{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.35 (from langchain)\n",
      "  Using cached langchain_core-0.3.37-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.3.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached SQLAlchemy-2.0.38-cp311-cp311-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Using cached aiohttp-3.11.12-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting numpy<2,>=1.26.4 (from langchain)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached frozenlist-1.5.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached multidict-6.1.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading propcache-0.3.0-cp311-cp311-win_amd64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached yarl-1.18.3-cp311-cp311-win_amd64.whl.metadata (71 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.35->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.12.2)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached orjson-3.10.15-cp311-cp311-win_amd64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached zstandard-0.23.0-cp311-cp311-win_amd64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached pydantic_core-2.27.2-cp311-cp311-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
      "  Using cached charset_normalizer-3.4.1-cp311-cp311-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-3.1.1-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Using cached langchain-0.3.19-py3-none-any.whl (1.0 MB)\n",
      "Using cached aiohttp-3.11.12-cp311-cp311-win_amd64.whl (442 kB)\n",
      "Using cached langchain_core-0.3.37-py3-none-any.whl (413 kB)\n",
      "Using cached langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
      "Using cached langsmith-0.3.10-py3-none-any.whl (333 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached SQLAlchemy-2.0.38-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp311-cp311-win_amd64.whl (102 kB)\n",
      "Using cached frozenlist-1.5.0-cp311-cp311-win_amd64.whl (51 kB)\n",
      "Using cached greenlet-3.1.1-cp311-cp311-win_amd64.whl (298 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached multidict-6.1.0-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Using cached orjson-3.10.15-cp311-cp311-win_amd64.whl (133 kB)\n",
      "Downloading propcache-0.3.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached yarl-1.18.3-cp311-cp311-win_amd64.whl (91 kB)\n",
      "Using cached zstandard-0.23.0-cp311-cp311-win_amd64.whl (495 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: zstandard, urllib3, tenacity, sniffio, PyYAML, pydantic-core, propcache, orjson, numpy, multidict, jsonpointer, idna, h11, greenlet, frozenlist, charset-normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, SQLAlchemy, requests, pydantic, jsonpatch, httpcore, anyio, aiosignal, requests-toolbelt, httpx, aiohttp, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.38 aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.8.0 attrs-25.1.0 certifi-2025.1.31 charset-normalizer-3.4.1 frozenlist-1.5.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.19 langchain-core-0.3.37 langchain-text-splitters-0.3.6 langsmith-0.3.10 multidict-6.1.0 numpy-1.26.4 orjson-3.10.15 propcache-0.3.0 pydantic-2.10.6 pydantic-core-2.27.2 requests-2.32.3 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.0.0 urllib3-2.3.0 yarl-1.18.3 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install langchain\n",
    "\n",
    "# !pip install -U langchain-community\n",
    "\n",
    "# !pip install pypdf\n",
    "\n",
    "# !pip install -U tqdm\n",
    "\n",
    "# !pip install -U ipywidgets  \n",
    "\n",
    "# !pip install -U sentence-transformers==2.2.2\n",
    "\n",
    "# !pip install torch transformers\n",
    "\n",
    "# !pip install InstructorEmbedding\n",
    "# pip install tqdm\n",
    "# !pip install --upgrade sentence-transformers huggingface-hub\n",
    "# pip install faiss-cpu\n",
    "# pip install --quiet langchain_experimental\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Whoosh\n",
      "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
      "Installing collected packages: Whoosh\n",
      "Successfully installed Whoosh-2.7.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (d:\\Dev\\LoTR_RAG\\.conda\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (d:\\Dev\\LoTR_RAG\\.conda\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (d:\\Dev\\LoTR_RAG\\.conda\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# pip install Whoosh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.chdir(\"../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import  RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from whoosh.index import create_in, open_dir\n",
    "from whoosh.fields import Schema, TEXT, ID, DATETIME, NUMERIC\n",
    "import datetime\n",
    "from whoosh.qparser import QueryParser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFDirectoryLoader(path=DATASET_PATH,\n",
    "                            recursive=True,\n",
    "                            silent_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documnets=loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Load DOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_documents = []\n",
    "empty_documents = []\n",
    "\n",
    "for document in documnets:\n",
    "    # Check if page content has words (not just numbers)\n",
    "    if re.search(r'\\w+', document.page_content):\n",
    "        page_content= document.page_content.replace('\\n',' ')\n",
    "        # Additional check: if content is only numbers and length < 3, consider empty\n",
    "        if not page_content.isdigit() and len(page_content.split()) >1:\n",
    "            non_empty_documents.append(document)\n",
    "        else:\n",
    "            empty_documents.append(document)\n",
    "    else:\n",
    "        empty_documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_27088\\55583320.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model=HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2',\n"
     ]
    }
   ],
   "source": [
    "embedding_model=HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2',\n",
    "                                      model_kwargs = {'device': device})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter_recursive = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_documents = text_splitter_recursive.split_documents(non_empty_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sementic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter_semantic=SemanticChunker(embeddings=embedding_model,\n",
    "                                       breakpoint_threshold_amount=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sementic_documnets=text_splitter_semantic.split_documents(non_empty_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates chunks\n",
    "def remove_duplicates(documents):\n",
    "    unique_documents = []\n",
    "    seen_texts = set()\n",
    "\n",
    "    for doc in documents:\n",
    "        if doc.page_content not in seen_texts:\n",
    "            unique_documents.append(doc)\n",
    "            seen_texts.add(doc.page_content)\n",
    "\n",
    "    return unique_documents\n",
    "\n",
    "unique_split_documents = remove_duplicates(split_documents)\n",
    "unique_sementic_documnets = remove_duplicates(sementic_documnets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fd7856970d47f28fff21080123e0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1e8bc0f4e149a7b9a2b8745b1f6a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d44a1713464c078c71130f69a44774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d102dd673649db87f97cb918173a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222e0dd88e0d48cab47e9268ed826477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed4192d04ff4806985dc0bd301d6b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding document splits: 100%|██████████| 14743/14743 [30:36<00:00,  8.03it/s] \n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "metadatas = []\n",
    "\n",
    "# Use tqdm to show progress\n",
    "for doc in tqdm(split_documents, desc=\"Embedding document splits\"):\n",
    "    # Tokenize the split text\n",
    "    encoded_input = tokenizer(doc.page_content, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embedding = F.normalize(embedding, p=2, dim=1)\n",
    "\n",
    "    # Save embedding and metadata\n",
    "    embeddings.append(embedding.squeeze().cpu().numpy())\n",
    "    metadatas.append(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save embeddings list\n",
    "with open('embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "# Save metadatas list\n",
    "with open('metadatas.pkl', 'wb') as f:\n",
    "    pickle.dump(metadatas, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybride_chunk = unique_split_documents+unique_sementic_documnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LangChain's FAISS wrapper\n",
    "vector_store = FAISS.from_documents(unique_sementic_documnets,embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.save_local(\"vector_db/faiss_index_hybride\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Word Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_WORD_BASE_INDEX_DIR=\"key_word_index\"\n",
    "if not os.path.exists(KEY_WORD_BASE_INDEX_DIR):\n",
    "    os.makedirs(KEY_WORD_BASE_INDEX_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a schema that includes your metadata fields\n",
    "schema = Schema(\n",
    "    title=TEXT(stored=True),          # e.g. \"The Two Towers\"\n",
    "    author=TEXT(stored=True),         # e.g. \"J. R. R. Tolkien\"\n",
    "    total_pages=NUMERIC(stored=True, numtype=int),  # e.g. 461\n",
    "    page=NUMERIC(stored=True, numtype=int),         # e.g. 0\n",
    "    page_label=TEXT(stored=True),  # e.g. \"C\"\n",
    "    content=TEXT(stored=True)    # The main content of the document\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = create_in(KEY_WORD_BASE_INDEX_DIR, schema)\n",
    "writer = ix.writer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing complete!\n"
     ]
    }
   ],
   "source": [
    "for doc in non_empty_documents:\n",
    "    \n",
    "    data={\n",
    "        \"title\": doc.metadata[\"title\"],\n",
    "        \"author\": doc.metadata[\"author\"],\n",
    "        \"total_pages\":int(doc.metadata[\"total_pages\"]),\n",
    "        \"page\": int(doc.metadata[\"page\"]) ,\n",
    "        \"page_label\": doc.metadata[\"page_label\"],\n",
    "        \"content\": doc.page_content \n",
    "    }\n",
    "    \n",
    "    writer.add_document(**data)\n",
    "\n",
    "# Commit the writer to save the documents.\n",
    "writer.commit()\n",
    "\n",
    "print(\"Indexing complete!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': 'J. R. R. Tolkien', 'content': 'viii contents\\nIV A Journey in the Dark 384\\nV The Bridge of Khazad-du ˆm 418\\nVI Lothlo ´rien 433\\nVII The Mirror of Galadriel 459\\nVIII Farewell to Lo ´rien 478\\nIX The Great River 495\\nX The Breaking of the Fellowship 515\\nMaps                                                            533\\nWorks By J.R.R. Tolkien                            540\\nAbout The Publisher                                         542\\nCopyright                                                    541', 'page': 6, 'page_label': 'vi', 'title': 'The Fellowship of the Ring', 'total_pages': 571}\n",
      "------\n",
      "{'author': 'J. R. R. Tolkien', 'content': 'xii contents\\nappendices\\na annals of the kings and rulers 1351\\nI The Nu´meno´rean Kings 1352\\nII The House of Eorl 1395\\nIII Durin’s Folk 1406\\nb the tale of years\\n(chronology of the westlands ) 1420\\nc family trees (hobbits ) 1444\\nd calendars 1451\\ne writing and spelling 1461\\nI Pronunciation of Words and Names 1461\\nII Writing 1467\\nf I The Languages and Peoples of the\\nThird Age 1480\\nII On Translation 1489\\nindexes\\nI Poems and Songs 1498\\nII Poems and Phrases in Languages\\nOther Than Common Speech 1500\\nIII Persons, Places and Things 1501\\nAbout The Publisher                               1578\\nCopyright                                                 1577\\nWorks By J.R.R. Tolkien                        1576\\nMaps                                                        1569', 'page': 9, 'page_label': 'xii', 'title': 'The Return of the King', 'total_pages': 620}\n",
      "------\n",
      "{'author': 'J. R. R. Tolkien', 'content': 'CONTENTS\\nSynopsis vii\\nbook five\\nI Minas Tirith 977\\nII The Passing of the Grey Company 1012\\nIII The Muster of Rohan 1035\\nIV The Siege of Gondor 1054\\nV The Ride of the Rohirrim 1086\\nVI The Battle of the Pelennor Fields 1098\\nVII The Pyre of Denethor 1113\\nVIII The Houses of Healing 1123\\nIX The Last Debate 1141\\nX The Black Gate Opens 1156\\nbook six\\nI The Tower of Cirith Ungol 1173\\nII The Land of Shadow 1198\\nIII Mount Doom 1220\\nIV The Field of Cormallen 1241\\nV The Steward and the King 1255\\nVI Many Partings 1275\\nVII Homeward Bound 1295\\nVIII The Scouring of the Shire 1306\\nIX The Grey Havens 1336', 'page': 8, 'page_label': 'xi', 'title': 'The Return of the King', 'total_pages': 620}\n",
      "------\n",
      "{'author': 'J. R. R. Tolkien', 'content': 'CONTENTS\\nNote on the Text                                                     ix\\nNote on the 50th Anniversary Edition                  xviii\\nForeword to the Second Edition                       xxiii\\nPrologue Concerning Hobbits, and other\\nmatters 1\\nbook one\\nI A Long-expected Party 27\\nII The Shadow of the Past 55\\nIII Three is Company 85\\nIV A Short Cut to Mushrooms 112\\nV A Conspiracy Unmasked 128\\nVI The Old Forest 143\\nVII In the House of Tom Bombadil 161\\nVIII Fog on the Barrow-downs 176\\nIX At the Sign of The Prancing Pony 195\\nX Strider 213\\nXI A Knife in the Dark 230\\nXII Flight to the Ford 257\\nbook two\\nI Many Meetings 285\\nII The Council of Elrond 311\\nIII The Ring Goes South 354', 'page': 5, 'page_label': 'v', 'title': 'The Fellowship of the Ring', 'total_pages': 571}\n",
      "------\n",
      "{'author': 'J. R. R. Tolkien', 'content': 'CONTENTS\\nSynopsis vii\\nbook three\\nI The Departure of Boromir 537\\nII The Riders of Rohan 547\\nIII The Uruk-hai 578\\nIV Treebeard 600\\nV The White Rider 636\\nVI The King of the Golden Hall 660\\nVII Helm’s Deep 686\\nVIII The Road to Isengard 708\\nIX Flotsam and Jetsam 730\\nX The Voice of Saruman 751\\nXI The Palantı ´r 767\\nbook four\\nI The Taming of Sme ´agol 787\\nII The Passage of the Marshes 810\\nIII The Black Gate is Closed 831\\nIV Of Herbs and Stewed Rabbit 847\\nV The Window on the West 866\\nVI The Forbidden Pool 893\\nVII Journey to the Cross-roads 908\\nVIII The Stairs of Cirith Ungol 920\\nIX Shelob’s Lair 938\\nX The Choices of Master Samwise 952\\nAbout The Publisher                       982\\nCopyright                                                         981\\nWorks By J.R.R. Tolkien                                 980\\nMaps                                                                 973', 'page': 9, 'page_label': 'ix', 'title': 'The Two Towers', 'total_pages': 461}\n",
      "------\n",
      "{'author': 'J. R. R. Tolkien', 'content': '1194 the return of the king\\n‘There’s something that might be useful,’ he said. ‘He’s\\ndead: the one that whipped you. Broke his neck, it seems, in\\nhis hurry. Now you draw up the ladder, if you can, Mr.\\nFrodo; and don’t you let it down till you hear me call the\\npass-word.Elbereth I’ll call. What the Elves say. No orc would\\nsay that.’\\nFrodo sat for a while and shivered, dreadful fears chasing\\none another through his mind. Then he got up, drew the\\ngrey elven-cloak about him, and to keep his mind occupied,\\nbegan to walk to and fro, prying and peering into every corner\\nof his prison.\\nIt was not very long, though fear made it seem an hour at\\nleast, before he heard Sam’s voice calling softly from below:\\nElbereth, Elbereth. Frodo let down the light ladder. Up came\\nSam, pufﬁng, heaving a great bundle on his head. He let it\\nfall with a thud.\\n‘Quick now, Mr. Frodo!’ he said. ‘I’ve had a bit of a search\\nto ﬁnd anything small enough for the likes of us. We’ll have\\nto make do. But we must hurry. I’ve met nothing alive, and\\nI’ve seen nothing, but I’m not easy. I think this place is being\\nwatched. I can’t explain it, but well: it feels to me as if one of\\nthose foul ﬂying Riders was about, up in the blackness where\\nhe can’t be seen.’\\nHe opened the bundle. Frodo looked in disgust at the\\ncontents, but there was nothing for it: he had to put the things\\non, or go naked. There were long hairy breeches of some\\nunclean beast-fell, and a tunic of dirty leather. He drew them\\non. Over the tunic went a coat of stout ring-mail, short for a\\nfull-sized orc, too long for Frodo and heavy. About it he\\nclasped a belt, at which there hung a short sheath holding\\na broad-bladed stabbing-sword. Sam had brought several\\norc-helmets. One of them ﬁtted Frodo well enough, a black\\ncap with iron rim, and iron hoops covered with leather upon\\nwhich the Evil Eye was painted in red above the beaklike\\nnose-guard.\\n‘The Morgul-stuff, Gorbag’s gear, was a better ﬁt and', 'page': 235, 'page_label': '1194', 'title': 'The Return of the King', 'total_pages': 620}\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# Open the index for querying.\n",
    "with ix.searcher() as searcher:\n",
    "    # Create a query parser for the \"content\" field.\n",
    "    parser = QueryParser(\"content\", ix.schema)\n",
    "    \n",
    "    # Parse your query text. Replace \"two towers\" with your actual query.\n",
    "    query = parser.parse(\"contents\")\n",
    "    \n",
    "    # Execute the search and limit to 10 results.\n",
    "    results = searcher.search(query, limit=10)\n",
    "    \n",
    "    # Iterate over the results and print out the document fields.\n",
    "    for hit in results:\n",
    "        print(hit.fields())\n",
    "        print(\"------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
